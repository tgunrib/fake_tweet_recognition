
@article{luo_efficient_2021,
	title = {Efficient English text classification using selected Machine Learning Techniques},
	volume = {60},
	issn = {1110-0168},
	url = {https://www.sciencedirect.com/science/article/pii/S1110016821000806},
	doi = {10.1016/j.aej.2021.02.009},
	abstract = {Text classification ({TC}) is an approach used for the classification of any kind of documents for the target category or out. In this paper, we implemented the Support Vector Machines ({SVM}) model in classifying English text and documents. Here we did two analytical experiments to check the selected classifiers using English documents. Experimental results performed on a set of 1033 text document present that the Rocchio classifier provides the best performance results when the size of the feature set is small while {SVM} outperforms the other classifiers. From the experimental analysis, we observed that the classification rate exceeds 90\% when using more than 4000 features.},
	pages = {3401--3409},
	number = {3},
	journaltitle = {Alexandria Engineering Journal},
	shortjournal = {Alexandria Engineering Journal},
	author = {Luo, Xiaoyu},
	urldate = {2022-12-21},
	date = {2021-06-01},
	langid = {english},
	keywords = {Machine Learning, English language, Support Vector Machines, Text classification, Text mining},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\tosin\\Zotero\\storage\\C6GB8X8J\\Luo - 2021 - Efficient English text classification using select.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\tosin\\Zotero\\storage\\AJY6M3TK\\S1110016821000806.html:text/html},
}

@article{dogra_complete_2022,
	title = {A Complete Process of Text Classification System Using State-of-the-Art {NLP} Models},
	volume = {2022},
	issn = {1687-5265},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9203176/},
	doi = {10.1155/2022/1883698},
	abstract = {With the rapid advancement of information technology, online information has been exponentially growing day by day, especially in the form of text documents such as news events, company reports, reviews on products, stocks-related reports, medical reports, tweets, and so on. Due to this, online monitoring and text mining has become a prominent task. During the past decade, significant efforts have been made on mining text documents using machine and deep learning models such as supervised, semisupervised, and unsupervised. Our area of the discussion covers state-of-the-art learning models for text mining or solving various challenging {NLP} (natural language processing) problems using the classification of texts. This paper summarizes several machine learning and deep learning algorithms used in text classification with their advantages and shortcomings. This paper would also help the readers understand various subtasks, along with old and recent literature, required during the process of text classification. We believe that readers would be able to find scope for further improvements in the area of text classification or to propose new techniques of text classification applicable in any domain of their interest.},
	pages = {1883698},
	journaltitle = {Computational Intelligence and Neuroscience},
	shortjournal = {Comput Intell Neurosci},
	author = {Dogra, Varun and Verma, Sahil and Kavita and Chatterjee, Pushpita and Shafi, Jana and Choi, Jaeyoung and Ijaz, Muhammad Fazal},
	urldate = {2022-12-21},
	date = {2022-06-09},
	pmid = {35720939},
	pmcid = {PMC9203176},
	file = {PubMed Central Full Text PDF:C\:\\Users\\tosin\\Zotero\\storage\\NRNGAE9V\\Dogra et al. - 2022 - A Complete Process of Text Classification System U.pdf:application/pdf},
}

@article{prabhakar_framework_2022,
	title = {A Framework for Text Classification Using Evolutionary Contiguous Convolutional Neural Network and Swarm Based Deep Neural Network},
	volume = {16},
	issn = {1662-5188},
	url = {https://www.frontiersin.org/articles/10.3389/fncom.2022.900885},
	abstract = {To classify the texts accurately, many machine learning techniques have been utilized in the field of Natural Language Processing ({NLP}). For many pattern classification applications, great success has been obtained when implemented with deep learning models rather than using ordinary machine learning techniques. Understanding the complex models and their respective relationships within the data determines the success of such deep learning techniques. But analyzing the suitable deep learning methods, techniques, and architectures for text classification is a huge challenge for researchers. In this work, a Contiguous Convolutional Neural Network ({CCNN}) based on Differential Evolution ({DE}) is initially proposed and named as Evolutionary Contiguous Convolutional Neural Network ({ECCNN}) where the data instances of the input point are considered along with the contiguous data points in the dataset so that a deeper understanding is provided for the classification of the respective input, thereby boosting the performance of the deep learning model. Secondly, a swarm-based Deep Neural Network ({DNN}) utilizing Particle Swarm Optimization ({PSO}) with {DNN} is proposed for the classification of text, and it is named Swarm {DNN}. This model is validated on two datasets and the best results are obtained when implemented with the Swarm {DNN} model as it produced a high classification accuracy of 97.32\% when tested on the {BBC} newsgroup text dataset and 87.99\% when tested on 20 newsgroup text datasets. Similarly, when implemented with the {ECCNN} model, it produced a high classification accuracy of 97.11\% when tested on the {BBC} newsgroup text dataset and 88.76\% when tested on 20 newsgroup text datasets.},
	journaltitle = {Frontiers in Computational Neuroscience},
	author = {Prabhakar, Sunil Kumar and Rajaguru, Harikumar and So, Kwangsub and Won, Dong-Ok},
	urldate = {2022-12-21},
	date = {2022},
	file = {Full Text PDF:C\:\\Users\\tosin\\Zotero\\storage\\CFK88KZ2\\Prabhakar et al. - 2022 - A Framework for Text Classification Using Evolutio.pdf:application/pdf},
}

@online{shaikh_machine_2017,
	title = {Machine Learning, {NLP}: Text Classification using scikit-learn, python and {NLTK}.},
	url = {https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a},
	shorttitle = {Machine Learning, {NLP}},
	abstract = {Latest Update:
I have uploaded the complete code (Python and Jupyter notebook) on {GitHub}: https://github.com/javedsha/text-classification},
	titleaddon = {Medium},
	author = {Shaikh, Javed},
	urldate = {2022-12-21},
	date = {2017-10-30},
	langid = {english},
	file = {Snapshot:C\:\\Users\\tosin\\Zotero\\storage\\PPKTXJVT\\machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a.html:text/html},
}

@article{ibtihel_semantic_2018,
	title = {A Semantic Approach for Tweet Categorization},
	volume = {126},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050918312432},
	doi = {10.1016/j.procs.2018.07.267},
	series = {Knowledge-Based and Intelligent Information \& Engineering Systems: Proceedings of the 22nd International Conference, {KES}-2018, Belgrade, Serbia},
	abstract = {The explosion of social media and microblogging services has gradually increased the microblogging data and particularly tweets data. In microblogging services such as Twitter, the users may become overwhelmed by the rise of data. Although, Twitter allows people to micro-blog about a broad range of topics in real time, it is often hard to understand what these tweets are about. In this work, we study the problem of Tweet Categorization ({TC}), which aims to automatically classify tweets based on their topic. The accurate {TC}, however, is a challenging task within the 140-character limit imposed by Twitter. The majority of {TC} approaches use lexical features such as Bag of Words ({BoW}) and Bag of Entities ({BoE}) extracted from a Tweet content. In this paper, we propose a semantic approach of improving the accuracy of {TC} based on feature expansion from external Knowledge Bases ({KBs}) and the use of {eXtended} {WordNet} Domain as a classifier. In particular, we propose a deep enrichment strategy to extend tweets with additional features by exploiting the concepts present in the semantic graph structures of the {KBs}. Then, our supervised categorization relies only on the ontological knowledge and classifier training is not required. Empirical results indicate that this enriched representation of text items can substantially improve the {TC} performance.},
	pages = {335--344},
	journaltitle = {Procedia Computer Science},
	shortjournal = {Procedia Computer Science},
	author = {Ibtihel, Ben Ltaifa and Lobna, Hlaoua and Maher, Ben Jemaa},
	urldate = {2022-12-21},
	date = {2018-01-01},
	langid = {english},
	keywords = {deep enrichment, {eXtended} {WordNet} Domain, Knowledge Bases ({KBs}), semantic approach, Tweet Categorization ({TC})},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\tosin\\Zotero\\storage\\I9HJRNVV\\Ibtihel et al. - 2018 - A Semantic Approach for Tweet Categorization.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\tosin\\Zotero\\storage\\STXDS3W5\\S1877050918312432.html:text/html},
}

@software{rebassoo_mediaeval_2022,
	title = {{MediaEval} 2015},
	url = {https://github.com/signerebassoo/MediaEval-2015/blob/90c9037d361439f8fbed7087c0fbfa7ef8356972/MediaEvalCW.ipynb},
	abstract = {Binary classification of the {MediaEval} 2015 Verifying Multimedia Use task dataset.},
	author = {Rebassoo, Signe},
	urldate = {2022-12-21},
	date = {2022-04-12},
	note = {original-date: 2021-03-25T12:03:33Z},
}

@article{boididou_verifying_nodate,
	title = {Verifying Multimedia Use at {MediaEval} 2015},
	abstract = {This paper provides an overview of the Verifying Multimedia Use task that takes places as part of the 2015 {MediaEval} Benchmark. The task deals with the automatic detection of manipulation and misuse of Web multimedia content. Its aim is to lay the basis for a future generation of tools that could assist media professionals in the process of veriﬁcation. Examples of manipulation include maliciously tampering with images and videos, e.g., splicing, removal/addition of elements, while other kinds of misuse include the reposting of previously captured multimedia content in a diﬀerent context (e.g., a new event) claiming that it was captured there. For the 2015 edition of the task, we have generated and made available a large corpus of real-world cases of images that were distributed through tweets, along with manually assigned labels regarding their use, i.e. misleading (fake) versus appropriate (real).},
	author = {Boididou, Christina and Andreadou, Katerina and Papadopoulos, Symeon and Dang-Nguyen, Duc-Tien and Boato, Giulia and Riegler, Michael and Kompatsiaris, Yiannis},
	langid = {english},
	file = {Boididou et al. - Verifying Multimedia Use at MediaEval 2015.pdf:C\:\\Users\\tosin\\Zotero\\storage\\AVV2P8XF\\Boididou et al. - Verifying Multimedia Use at MediaEval 2015.pdf:application/pdf},
}

@article{boididou_detection_2018,
	title = {Detection and visualization of misleading content on Twitter},
	volume = {7},
	issn = {2192-662X},
	url = {https://doi.org/10.1007/s13735-017-0143-x},
	doi = {10.1007/s13735-017-0143-x},
	abstract = {The problems of online misinformation and fake news have gained increasing prominence in an age where user-generated content and social media platforms are key forces in the shaping and diffusion of news stories. Unreliable information and misleading content are often posted and widely disseminated through popular social media platforms such as Twitter and Facebook. As a result, journalists and editors are in need of new tools that can help them speed up the verification process for content that is sourced from social media. Motivated by this need, in this paper, we present a system that supports the automatic classification of multimedia Twitter posts into credible or misleading. The system leverages credibility-oriented features extracted from the tweet and the user who published it, and trains a two-step classification model based on a novel semisupervised learning scheme. The latter uses the agreement between two independent pretrained models on new posts as guiding signals for retraining the classification model. We analyze a large labeled dataset of tweets that shared debunked fake and confirmed real images and videos, and show that integrating the newly proposed features, and making use of bagging in the initial classifiers and of the semisupervised learning scheme, significantly improves classification accuracy. Moreover, we present a Web-based application for visualizing and communicating the classification results to end users.},
	pages = {71--86},
	number = {1},
	journaltitle = {International Journal of Multimedia Information Retrieval},
	shortjournal = {Int J Multimed Info Retr},
	author = {Boididou, Christina and Papadopoulos, Symeon and Zampoglou, Markos and Apostolidis, Lazaros and Papadopoulou, Olga and Kompatsiaris, Yiannis},
	urldate = {2022-12-21},
	date = {2018-03-01},
	langid = {english},
	keywords = {Fake detection, Information credibility, Social media, Verification},
	file = {Full Text PDF:C\:\\Users\\tosin\\Zotero\\storage\\UK2HGWTS\\Boididou et al. - 2018 - Detection and visualization of misleading content .pdf:application/pdf},
}

@article{jin_mcg-ict_nodate,
	title = {{MCG}-{ICT} at {MediaEval} 2015: Verifying Multimedia Use with a Two-Level Classiﬁcation Model},
	abstract = {The Verifying Multimedia Use task aims to detect misuse of online multimedia content and verify them as real or fake. This is a highly challenging problem because of strong variations among tweets from diﬀerent events. Traditional approaches train the classiﬁer at message level, which ignores inter-message relations. We propose a two-level classiﬁcation model to exploit the information that tweets of a same topic are probably have same credibility values. In this model a topic level is introduced to eliminate message variations. Messages are aggregated into topics as a higher level representation. Pre-results gained from classiﬁcation at the topic level are then fused with original message level features to train a better classiﬁer. Results indicate that topic level is very helpful and our two-level approach oﬀers signiﬁcantly better results than a traditional one-level method. Our best result on this task achieves an F-score of 0.94 using features extracted only from tweet content.},
	author = {Jin, Zhiwei and Cao, Juan and Zhang, Yazi and Zhang, Yongdong},
	langid = {english},
	file = {Jin et al. - MCG-ICT at MediaEval 2015 Verifying Multimedia Us.pdf:C\:\\Users\\tosin\\Zotero\\storage\\842DUKCU\\Jin et al. - MCG-ICT at MediaEval 2015 Verifying Multimedia Us.pdf:application/pdf},
}

@online{noauthor_misinformation_nodate,
	title = {Misinformation definition and meaning {\textbar} Collins English Dictionary},
	url = {https://www.collinsdictionary.com/dictionary/english/misinformation},
	abstract = {Misinformation definition: Misinformation is wrong information which is given to someone, often in a deliberate... {\textbar} Meaning, pronunciation, translations and examples},
	urldate = {2022-12-21},
	langid = {english},
	file = {Snapshot:C\:\\Users\\tosin\\Zotero\\storage\\BLTIJNAZ\\misinformation.html:text/html},
}

@online{li_complete_2019,
	title = {A Complete Exploratory Data Analysis and Visualization for Text Data},
	url = {https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a},
	abstract = {How to combine visualization and {NLP} in order to generate insights in an intuitive way},
	titleaddon = {Medium},
	author = {Li, Susan},
	urldate = {2022-12-21},
	date = {2019-04-27},
	langid = {english},
}

@article{luo_spread_2021,
	title = {Spread of Misinformation in Social Networks: Analysis Based on Weibo Tweets},
	volume = {2021},
	issn = {1939-0114},
	url = {https://www.hindawi.com/journals/scn/2021/7999760/},
	doi = {10.1155/2021/7999760},
	shorttitle = {Spread of Misinformation in Social Networks},
	abstract = {Social networks are filled with a large amount of misinformation, which often misleads the public to make wrong decisions, stimulates negative public emotions, and poses serious threats to public safety and social order. The spread of misinformation in social networks has also become a widespread concern among scholars. In the study, we took the misinformation spread on social media as the research object and compared it with true information to better understand the characteristics of the spread of misinformation in social networks. This study adopts a deep learning method to perform content analysis and emotion analysis on misinformation dataset and true information dataset and adopts an analytic network process to analyze the differences between misinformation and true information in terms of network diffusion characteristics. The research findings reveal that the spread of misinformation on social media is influenced by content features and different emotions and consequently produces different changes. The related research findings enrich the existing research and make a certain contribution to the governance of misinformation and the maintenance of network order.},
	pages = {e7999760},
	journaltitle = {Security and Communication Networks},
	author = {Luo, Han and Cai, Meng and Cui, Ying},
	urldate = {2022-12-21},
	date = {2021-12-16},
	langid = {english},
	note = {Publisher: Hindawi},
	file = {Full Text PDF:C\:\\Users\\tosin\\Zotero\\storage\\RV24SDZZ\\Luo et al. - 2021 - Spread of Misinformation in Social Networks Analy.pdf:application/pdf},
}

@online{noauthor_why_nodate,
	title = {Why People Are Better At Lying Online Than Telling A Lie Face-to-face},
	url = {https://www.sciencedaily.com/releases/2009/05/090503203738.htm},
	abstract = {In the digital world, it's easier to tell a lie and get away with it. That's good news for liars, but not so good for anyone being deceived.},
	titleaddon = {{ScienceDaily}},
	urldate = {2022-12-21},
	langid = {english},
}

@article{etaiwi_impact_2017,
	title = {The Impact of applying Different Preprocessing Steps on Review Spam Detection},
	volume = {113},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050917317787},
	doi = {10.1016/j.procs.2017.08.368},
	abstract = {Online reviews become a valuable source of information that indicate the overall opinion about products and services, which {aOfnfelicntecruesvtoiemwesr}’bsecdoemciesioanvatoluapbulrechsaosuercae porfoidnufcotrmoartisoenrvtihcaet. {iSnidnicceatenotht} ealolvoernallilneoprienvioienwasbaonudt pcroomdumcetsntasndaresertvruicthefsu, lw, hiticihs iamffpecotrtacnutsttoomdeert’esctdefcaikseioanndtoppouisrocnharseeviaewpsr.{odMuacnt} yormsaecrhviincee.{leSairnncinegntoetchanlliqounelsinceoureldvibewe} sapapnldiedcotmo mdeetnetcst asrpeamtrurtehvfiuelw, sit biys eimxtpraocrttainngt taoudseefteucltfefaaktuereasnfdropmoisreovnierwev’isewtesx.t Musainngy Nmaatcuhrainl {eLalenagrunaingge} Ptercohcneisqsuinegs ({cNoLulPd}).{bMe} aanpypltiyedpetsoodfefteeacttursepsamcoureldvibeewussbeyd ienxttrhaicstimnganaoursesufuclhfeaastulirnegsufirsotimc freeavtiuerwe’ss, {tWexotrudsiCnoguNnta},tunr-{aglraLmanfgeuaatugreePsreotcsesasnidngn}({uNmLbPe})r. {oMf} apnryontoyupness.{oIfnfeoartduerrestocoeuxltdrabcet} suuscehd ifneatthuirsesm, manaonrystuycphesaosflipnrgeupirsotcicesfseinatgursetesp, {sWcoourdldCboeupnet},rfno-rgmraemd bfeefaotruereapspeltysinagndthneucmlabsesrifiocfatpioronnmouenths.{odIn}, thoirsdesrtetpos emxatryacintcsluucdhe {PfeOatSurteasg},gminagn,ynt-ygpraems otfeprmrepfrreoqcuesesnicnigess,tesptesmcmouilndg,bestpoeprfwoormrdedanbdefpournecatupaptliyoinngmtahrekcslafislsteifriicnagt,ioentcm. tehthisodp,rethpirsocsetespsisnmg asyteipnscmludaye {aPfOfeScttatghgeinogv},erna-lgl raacmcutrearcmy forfeqthueenrceiveise,wstesmpammindge,tesctotiponwtoarsdk.{anInd} tphuisncrteusaetaiorcnh,mwarekswfiilllteinrivnegs,tiegtact.eththise perfefpecrotsceosfsipnrgepsrtoecpessmsinagy satfefpecstotnhethoevaecraclulraacccyuorafcryevoifewthsesrpeavmiewdetsepcatmiond.{eDteicftfieornenttasmka}. {cIhnintheisleraersneinargcha},lgworeitwhmills {iwnvilelsbtiegaatpeptlhieedesfufecchtsasofSpurpepporrotcVesiscitnogr} Msteapcshionne t({hSeVaMcc}){uarnacdyNoafïvreevBieawyesss}({pNamB}),deatnedctaiolna.{beDleifdfedraetnatsemtaocfhHinoetelelsarrneivnigewaslgworiiltlhbmesanwaillyl} zbeeaanpdpplireodcessusc.{hTahseSeuffpipcioernt} {cVyicwtiolrl} Mbeaecvhainluea({teSdVaMcc}){oarnddinNg} atoïvme {aBnayyeesva}({NluBat})i,oannmd eaalsaubreelsesdudchataasse: {tporefcHisoiotenl},srreecvailelwansdwaicllcubreaacnya. lyze and process. The efficiency will be evaluated according to many evaluation measures such as: precision, recall and accuracy. ©{PPP}©©eeee2ee22r0rr00---1r11rre7ee77vvvTTTiiieeehhhwwweeeAuAAuunnnuuudddttthehheeorrroorrrrrrsessee.s..{ssPpppPPouoouunbnnbbslsslliiiiiisbssbbhhhiiilelleeiiidtddttyyybbboooyyyfffEEEttthhhlllsesseeeeeCvCCvvioiiooeeenrnnrrfffBeBBeer}.rr..{VeeeVVnnn}...ccceee {PPPrrrooogggrrraaammm} {CCChhhaaaiiirrrsss}...},
	pages = {273--279},
	journaltitle = {Procedia Computer Science},
	shortjournal = {Procedia Computer Science},
	author = {Etaiwi, Wael and Naymat, Ghazi},
	urldate = {2022-12-21},
	date = {2017},
	langid = {english},
	file = {Etaiwi and Naymat - 2017 - The Impact of applying Different Preprocessing Ste.pdf:C\:\\Users\\tosin\\Zotero\\storage\\HVF4UCVZ\\Etaiwi and Naymat - 2017 - The Impact of applying Different Preprocessing Ste.pdf:application/pdf},
}

@online{singh_countvectorizer_2022,
	title = {{CountVectorizer} vs {TfidfVectorizer}},
	url = {https://medium.com/@shandeep92/countvectorizer-vs-tfidfvectorizer-cf62d0a54fa4},
	abstract = {Introduction to Text Feature Extraction},
	titleaddon = {Medium},
	author = {Singh, Shandeep},
	urldate = {2023-01-06},
	date = {2022-01-31},
	langid = {english},
	file = {Snapshot:C\:\\Users\\tosin\\Zotero\\storage\\MZ5NK9UL\\countvectorizer-vs-tfidfvectorizer-cf62d0a54fa4.html:text/html},
}

@online{noauthor_loss_nodate,
	title = {Loss Functions — {ML} Glossary documentation},
	url = {https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html},
	urldate = {2023-01-07},
}

@online{varma_picking_nodate,
	title = {Picking Loss Functions - A comparison between {MSE}, Cross Entropy, and Hinge Loss},
	url = {https://rohanvarma.me/Loss-Functions/},
	author = {Varma, Rohan},
	urldate = {2023-01-07},
	file = {Snapshot:C\:\\Users\\tosin\\Zotero\\storage\\D68Q8NFZ\\Loss-Functions.html:text/html},
}

@inreference{noauthor_logistic_2022,
	title = {Logistic regression},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Logistic_regression&oldid=1129858286},
	abstract = {In statistics, the logistic model (or logit model) is a statistical model that models the probability of an event taking place by having the log-odds for the event be a linear combination of one or more independent variables. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (the coefficients in the linear combination). Formally, in binary logistic regression there is a single binary dependent variable, coded by an indicator variable, where the two values are labeled "0" and "1", while the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled "1" can vary between 0 (certainly the value "0") and 1 (certainly the value "1"), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. See § Background and § Definition for formal mathematics, and § Example for a worked example.
Binary variables are widely used in statistics to model the probability of a certain class or event taking place, such as the probability of a team winning, of a patient being healthy, etc. (see § Applications), and the logistic model has been the most commonly used model for binary regression since about 1970. Binary variables can be generalized to categorical variables when there are more than two possible values (e.g. whether an image is of a cat, dog, lion, etc.), and the binary logistic regression generalized to multinomial logistic regression. If the multiple categories are ordered, one can use the ordinal logistic regression (for example the proportional odds ordinal logistic model). See § Extensions for further extensions. The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier.
Analogous linear models for binary variables with a different sigmoid function instead of the logistic function (to convert the linear combination to a probability) can also be used, most notably the probit model; see § Alternatives. The defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio. More abstractly, the logistic function is the natural parameter for the Bernoulli distribution, and in this sense is the "simplest" way to convert a real number to a probability. In particular, it maximizes entropy (minimizes added information), and in this sense makes the fewest assumptions of the data being modeled; see § Maximum entropy.
The parameters of a logistic regression are most commonly estimated by maximum-likelihood estimation ({MLE}). This does not have a closed-form expression, unlike linear least squares; see § Model fitting. Logistic regression by {MLE} plays a similarly basic role for binary or categorical responses as linear regression by ordinary least squares ({OLS}) plays for scalar responses: it is a simple, well-analyzed baseline model; see § Comparison with linear regression for discussion. The logistic regression as a general statistical model was originally developed and popularized primarily by Joseph Berkson, beginning in Berkson (1944), where he coined "logit"; see § History.},
	booktitle = {Wikipedia},
	urldate = {2023-01-07},
	date = {2022-12-27},
	langid = {english},
	note = {Page Version {ID}: 1129858286},
	file = {Snapshot:C\:\\Users\\tosin\\Zotero\\storage\\Y4C2EEIB\\Logistic_regression.html:text/html},
}

@inproceedings{makalic_review_2011,
	location = {Berlin, Heidelberg},
	title = {Review of Modern Logistic Regression Methods with Application to Small and Medium Sample Size Problems},
	isbn = {978-3-642-17432-2},
	doi = {10.1007/978-3-642-17432-2_22},
	series = {Lecture Notes in Computer Science},
	abstract = {Logistic regression is one of the most widely applied machine learning tools in binary classification problems. Traditionally, inference of logistic models has focused on stepwise regression procedures which determine the predictor variables to be included in the model. Techniques that modify the log-likelihood by adding a continuous penalty function of the parameters have recently been used when inferring logistic models with a large number of predictor variables. This paper compares and contrasts three popular penalized logistic regression methods: ridge regression, the Least Absolute Shrinkage and Selection Operator ({LASSO}) and the elastic net. The methods are compared in terms of prediction accuracy using simulated data as well as real data sets.},
	pages = {213--222},
	booktitle = {{AI} 2010: Advances in Artificial Intelligence},
	publisher = {Springer},
	author = {Makalic, Enes and Schmidt, Daniel Francis},
	editor = {Li, Jiuyong},
	date = {2011},
	langid = {english},
	keywords = {{LASSO}, Elastic Net, Logistic regression, Ridge regression, Variable Selection},
	file = {Full Text PDF:C\:\\Users\\tosin\\Zotero\\storage\\KRLUMGPV\\Makalic and Schmidt - 2011 - Review of Modern Logistic Regression Methods with .pdf:application/pdf},
}

@online{ratz_multinomial_2022,
	title = {Multinomial Naїve Bayes’ For Documents Classification and Natural Language Processing ({NLP})},
	url = {https://towardsdatascience.com/multinomial-na%C3%AFve-bayes-for-documents-classification-and-natural-language-processing-nlp-e08cc848ce6},
	abstract = {Multinomial naïve Bayes’ implementation in Python 3.8, {NumPy} and {NLTK}},
	titleaddon = {Medium},
	author = {Ratz, Arthur V.},
	urldate = {2023-01-07},
	date = {2022-04-08},
	langid = {english},
	file = {Snapshot:C\:\\Users\\tosin\\Zotero\\storage\\KQHBHEGN\\multinomial-naïve-bayes-for-documents-classification-and-natural-language-processing-nlp-e08cc.html:text/html},
}

@online{krukowski_how_2022,
	title = {How to translate languages in Python with Google Translate and {DeepL} (plus more)},
	url = {https://lokalise.com/blog/how-to-translate-languages-in-python-with-google-translate-and-deepl-plus-more/},
	abstract = {Looking to easily translate nearly any language in Python? This tutorial takes you through the steps to translating strings in Python.},
	titleaddon = {Lokalise Blog},
	author = {Krukowski, Ilya},
	urldate = {2023-01-09},
	date = {2022-10-30},
	langid = {american},
	file = {Snapshot:C\:\\Users\\tosin\\Zotero\\storage\\ZN8EMISN\\how-to-translate-languages-in-python-with-google-translate-and-deepl-plus-more.html:text/html},
}

@online{wolfe_many_2022,
	title = {Many Languages, One Deep Learning Model},
	url = {https://towardsdatascience.com/many-languages-one-deep-learning-model-69201d02dee1},
	abstract = {Multilingual understanding is easier than you think!},
	titleaddon = {Medium},
	author = {Wolfe, Cameron},
	urldate = {2023-01-09},
	date = {2022-11-01},
	langid = {english},
}

@article{dashtipour_multilingual_2016,
	title = {Multilingual Sentiment Analysis: State of the Art and Independent Comparison of Techniques},
	volume = {8},
	issn = {1866-9956},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4981629/},
	doi = {10.1007/s12559-016-9415-7},
	shorttitle = {Multilingual Sentiment Analysis},
	abstract = {With the advent of Internet, people actively express their opinions about products, services, events, political parties, etc., in social media, blogs, and website comments. The amount of research work on sentiment analysis is growing explosively. However, the majority of research efforts are devoted to English-language data, while a great share of information is available in other languages. We present a state-of-the-art review on multilingual sentiment analysis. More importantly, we compare our own implementation of existing approaches on common data. Precision observed in our experiments is typically lower than the one reported by the original authors, which we attribute to the lack of detail in the original presentation of those approaches. Thus, we compare the existing works by what they really offer to the reader, including whether they allow for accurate implementation and for reliable reproduction of the reported results.},
	pages = {757--771},
	journaltitle = {Cognitive Computation},
	shortjournal = {Cognit Comput},
	author = {Dashtipour, Kia and Poria, Soujanya and Hussain, Amir and Cambria, Erik and Hawalah, Ahmad Y. A. and Gelbukh, Alexander and Zhou, Qiang},
	urldate = {2023-01-09},
	date = {2016},
	pmid = {27563360},
	pmcid = {PMC4981629},
	file = {PubMed Central Full Text PDF:C\:\\Users\\tosin\\Zotero\\storage\\GSRT22AX\\Dashtipour et al. - 2016 - Multilingual Sentiment Analysis State of the Art .pdf:application/pdf},
}

@online{noauthor_lemmatization_nodate,
	title = {lemmatization},
	url = {https://dictionary.cambridge.org/dictionary/english/lemmatization},
	abstract = {1. the process of reducing the different forms of a word to one single form…},
	urldate = {2023-01-09},
	langid = {english},
}

@inreference{noauthor_stemming_2022,
	title = {Stemming},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Stemming&oldid=1108161550},
	abstract = {In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. Algorithms for stemming have been studied in computer science since the 1960s. Many search engines treat words with the same stem as synonyms as a kind of query expansion, a process called conflation.
A computer program or subroutine that stems word may be called a stemming program, stemming algorithm, or stemmer.},
	booktitle = {Wikipedia},
	urldate = {2023-01-09},
	date = {2022-09-02},
	langid = {english},
	note = {Page Version {ID}: 1108161550},
}

@online{yse_text_2021,
	title = {Text Normalization for Natural Language Processing ({NLP})},
	url = {https://towardsdatascience.com/text-normalization-for-natural-language-processing-nlp-70a314bfa646},
	abstract = {Stemming and lemmatization with Python},
	titleaddon = {Medium},
	author = {Yse, Diego Lopez},
	urldate = {2023-01-09},
	date = {2021-03-12},
	langid = {english},
}

@article{rianto_improving_2021,
	title = {Improving the accuracy of text classification using stemming method, a case of non-formal Indonesian conversation},
	volume = {8},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-021-00413-1},
	doi = {10.1186/s40537-021-00413-1},
	abstract = {Stemming has long been used in data pre-processing to retrieve information by tracking affixed words back into their root. In an Indonesian setting, existing stemming methods have been observed, and the existing stemming methods are proven to result in high accuracy level. However, there are not many stemming methods for non-formal Indonesian text processing. This study introduces a new stemming method to solve problems in the non-formal Indonesian text data pre-processing. Furthermore, this study aims to improve the accuracy of text classifier models by strengthening stemming method. Using the Support Vector Machine algorithm, a text classifier model is developed, and its accuracy is checked. The experimental evaluation was done by testing 550 datasets in Indonesian using two different stemming methods.},
	pages = {26},
	number = {1},
	journaltitle = {Journal of Big Data},
	shortjournal = {Journal of Big Data},
	author = {{Rianto} and Mutiara, Achmad Benny and Wibowo, Eri Prasetyo and Santosa, Paulus Insap},
	urldate = {2023-01-09},
	date = {2021-01-29},
	keywords = {Accuracy, Classification, Indonesian, Stemming, Text processing},
	file = {Full Text PDF:C\:\\Users\\tosin\\Zotero\\storage\\SBHNPM9W\\Rianto et al. - 2021 - Improving the accuracy of text classification usin.pdf:application/pdf;Snapshot:C\:\\Users\\tosin\\Zotero\\storage\\RB2W8W2S\\s40537-021-00413-1.html:text/html},
}

@article{rakhmanov_comparative_2020,
	title = {A Comparative Study on Vectorization and Classification Techniques in Sentiment Analysis to Classify Student-Lecturer Comments},
	volume = {178},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050920323954},
	doi = {10.1016/j.procs.2020.11.021},
	series = {9th International Young Scientists Conference in Computational Science, {YSC}2020, 05-12 September 2020},
	abstract = {Sentiment analysis is one of the important fields in educational data mining. In this paper, a large dataset, more than 52 000 comments, was used during experiment to develop a state-of-art classification model. The correlation test was conducted on sentiment analysis results and scale-rated survey results, and the result (r(203)=.79, p{\textless}.001) shows that sentiment analysis can be accepted as reasonable method for course and lecturer evaluation. A comparative analysis was done between different vectorization and classification techniques. The results of the experiment show that classifier built using Random Forest was most optimal and efficient classification model with state-of-art prediction accuracy of 97\% for 3-class classification. Moreover, to improve the diversity of the comments, a 5-class dataset was formed and experiment resulted with an efficient classification model with accuracy of 92\%. The Tf-Idf vectorization technique performed better than Count (Binary) vectorization.},
	pages = {194--204},
	journaltitle = {Procedia Computer Science},
	shortjournal = {Procedia Computer Science},
	author = {Rakhmanov, Ochilbek},
	urldate = {2023-01-09},
	date = {2020-01-01},
	langid = {english},
	keywords = {comment classification, Sentiment analysis, vectorization},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\tosin\\Zotero\\storage\\J94F86HL\\Rakhmanov - 2020 - A Comparative Study on Vectorization and Classific.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\tosin\\Zotero\\storage\\D44N5JPL\\S1877050920323954.html:text/html},
}

@online{borcan_tf-idf_2020,
	title = {{TF}-{IDF} Explained And Python Sklearn Implementation},
	url = {https://towardsdatascience.com/tf-idf-explained-and-python-sklearn-implementation-b020c5e83275},
	abstract = {What is {TF}-{IDF} and how you can implement it in Python and Scikit-Learn.},
	titleaddon = {Medium},
	author = {Borcan, Marius},
	urldate = {2023-01-09},
	date = {2020-06-08},
	langid = {english},
	file = {Snapshot:C\:\\Users\\tosin\\Zotero\\storage\\YGH5UQFB\\tf-idf-explained-and-python-sklearn-implementation-b020c5e83275.html:text/html},
}

@online{ratz_multinomial_2022-1,
	title = {Multinomial Naїve Bayes’ For Documents Classification and Natural Language Processing ({NLP})},
	url = {https://towardsdatascience.com/multinomial-na%C3%AFve-bayes-for-documents-classification-and-natural-language-processing-nlp-e08cc848ce6},
	abstract = {Multinomial naïve Bayes’ implementation in Python 3.8, {NumPy} and {NLTK}},
	titleaddon = {Medium},
	author = {Ratz, Arthur V.},
	urldate = {2023-01-09},
	date = {2022-04-08},
	langid = {english},
	file = {Snapshot:C\:\\Users\\tosin\\Zotero\\storage\\DFLP2XP3\\multinomial-naïve-bayes-for-documents-classification-and-natural-language-processing-nlp-e08cc.html:text/html},
}

@online{noauthor_loss_nodate-1,
	title = {Loss Functions — {ML} Glossary documentation},
	url = {https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html},
	urldate = {2023-01-09},
	file = {Loss Functions — ML Glossary documentation:C\:\\Users\\tosin\\Zotero\\storage\\VVISVECP\\loss_functions.html:text/html},
}

@online{noauthor_cost_nodate,
	title = {Cost Function {\textbar} Types of Cost Function Machine Learning},
	url = {https://www.analyticsvidhya.com/blog/2021/02/cost-function-is-no-rocket-science/},
	urldate = {2023-01-09},
	file = {Cost Function | Types of Cost Function Machine Learning:C\:\\Users\\tosin\\Zotero\\storage\\I5HLZCUU\\cost-function-is-no-rocket-science.html:text/html},
}

@inreference{noauthor_support_2022,
	title = {Support vector machine},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Support_vector_machine&oldid=1129233078},
	abstract = {In machine learning, support vector machines ({SVMs}, also support vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at {AT}\&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) {SVMs} are one of the most robust prediction methods, being based on statistical learning frameworks or {VC} theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an {SVM} training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use {SVM} in a probabilistic classification setting). {SVM} maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.
In addition to performing linear classification, {SVMs} can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data.},
	booktitle = {Wikipedia},
	urldate = {2023-01-09},
	date = {2022-12-24},
	langid = {english},
	note = {Page Version {ID}: 1129233078},
	file = {Snapshot:C\:\\Users\\tosin\\Zotero\\storage\\F949DAX7\\Support_vector_machine.html:text/html},
}
