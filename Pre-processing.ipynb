{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweet-preprocessor in c:\\users\\tosin\\miniconda3\\lib\\site-packages (0.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tosin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tosin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\tosin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install tweet-preprocessor\n",
    "import pandas as pd\n",
    "import preprocessor as p\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from googletrans import Translator\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_download(filename):\n",
    "    data = pd.read_table(filename, sep=\"\\t\",header=0).dropna()\n",
    "    print(data.head())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "# Functions to clean tweets\n",
    "def remove_links(tweet):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)   # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet)  # remove bitly links\n",
    "    tweet = tweet.strip('[link]')   # remove [links]\n",
    "    tweet = re.sub(r'pic.twitter\\S+','', tweet)\n",
    "    return tweet\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def preprocess_data(data):\n",
    "    translator = Translator()\n",
    "    translator.raise_Exception = True\n",
    "    p.set_options(p.OPT.URL,p.OPT.EMOJI,p.OPT.RESERVED,p.OPT.MENTION)\n",
    "    punctuation = string.punctuation.replace(\"#\",\"\").replace(\"@\",\"\")\n",
    "    #p.set_options(p.OPT.URL,p.OPT.EMOJI,p.OPT.RESERVED)\n",
    "    data.tweetText = data.tweetText.apply(lambda x : translator.translate(x,dest='en').text)\n",
    "    data.tweetText = data.tweetText.apply(lambda x : (p.clean(remove_links(x)).replace('\\\\n','').lower().translate(str.maketrans('', '', punctuation))))\n",
    "    data.drop(columns=['username', 'timestamp','userId','imageId(s)'])\n",
    "    print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def lemmatise_data(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data['tokens'] = data.tweetText.apply(lambda x : [lemmatizer.lemmatize(x) for x in word_tokenize(x)])\n",
    "    data['label'] = data.label.apply(lambda  x : 'fake' if x == 'humor' else x )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def download_train_data():\n",
    "    train_data = data_download('assignment-comp3222-comp6246-mediaeval2015-dataset/mediaeval-2015-trainingset.txt')\n",
    "    preprocess_data(train_data)\n",
    "    lemmatise_data(train_data)\n",
    "    train_data.to_csv('train_data.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def download_test_data():\n",
    "    train_data = data_download('assignment-comp3222-comp6246-mediaeval2015-dataset/mediaeval-2015-testset.txt')\n",
    "    preprocess_data(train_data)\n",
    "    lemmatise_data(train_data)\n",
    "    train_data.to_csv('test_data.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              tweetId                                          tweetText  \\\n",
      "0  263046056240115712  ¿Se acuerdan de la película: “El día después d...   \n",
      "1  262995061304852481  @milenagimon: Miren a Sandy en NY!  Tremenda i...   \n",
      "2  262979898002534400  Buena la foto del Huracán Sandy, me recuerda a...   \n",
      "3  262996108400271360     Scary shit #hurricane #NY http://t.co/e4JLBUfH   \n",
      "4  263018881839411200  My fave place in the world #nyc #hurricane #sa...   \n",
      "\n",
      "      userId      imageId(s)        username                       timestamp  \\\n",
      "0   21226711  sandyA_fake_46         iAnnieM  Mon Oct 29 22:34:01 +0000 2012   \n",
      "1  192378571  sandyA_fake_09  CarlosVerareal  Mon Oct 29 19:11:23 +0000 2012   \n",
      "2  132303095  sandyA_fake_09     LucasPalape  Mon Oct 29 18:11:08 +0000 2012   \n",
      "3  241995902  sandyA_fake_29     Haaaaarryyy  Mon Oct 29 19:15:33 +0000 2012   \n",
      "4  250315890  sandyA_fake_15  princess__natt  Mon Oct 29 20:46:02 +0000 2012   \n",
      "\n",
      "  label  \n",
      "0  fake  \n",
      "1  fake  \n",
      "2  fake  \n",
      "3  fake  \n",
      "4  fake  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "download_train_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "download_test_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
